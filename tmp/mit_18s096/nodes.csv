id,label,item_id,kind,course_id,section_index,section_slug,section_title,section_chunk_index,week,order,tags,source_path,metrics
0,2x2Jacobians.jl,2x2jacobians_jl,concept,18.s096-iap-2023,5,resources,Resources,7,5,0,,resources/2x2jacobians_jl,{}
1,AutoDiff.ipynb,autodiff_ipynb,concept,18.s096-iap-2023,5,resources,Resources,7,5,1,,resources/autodiff_ipynb,{}
2,determinant_and_inverse.jl,determinant_and_inverse_jl,concept,18.s096-iap-2023,5,resources,Resources,7,5,2,,resources/determinant_and_inverse_jl,{}
3,fd_checks.ipynb,fd_checks_ipynb,concept,18.s096-iap-2023,5,resources,Resources,7,5,3,,resources/fd_checks_ipynb,{}
4,Lecture 1, Part 1: Introduction to Matrix Calculus ,mit18_s096iap23_lec01_pdf,lecture,18.s096-iap-2023,5,resources,Resources,0,1,4,,resources/mit18_s096iap23_lec01_pdf,{}
5,Lecture 1, Part 2: Derivatives as Linear Operators,mit18_s096iap23_lec02_pdf,lecture,18.s096-iap-2023,5,resources,Resources,0,2,5,,resources/mit18_s096iap23_lec02_pdf,{}
6,Lecture 2, Part 2: Jacobians of Matrix Functions,mit18_s096iap23_lec03_pdf,lecture,18.s096-iap-2023,5,resources,Resources,0,3,6,,resources/mit18_s096iap23_lec03_pdf,{}
7,Lecture 3, Part 2: Finite-Difference Approximations,mit18_s096iap23_lec04_pdf,lecture,18.s096-iap-2023,5,resources,Resources,0,4,7,,resources/mit18_s096iap23_lec04_pdf,{}
8,Lecture 4, Part 1: Derivatives in General Vector Spaces,mit18_s096iap23_lec05_pdf,lecture,18.s096-iap-2023,5,resources,Resources,0,5,8,,resources/mit18_s096iap23_lec05_pdf,{}
9,Lecture 4, Part 2: Nonlinear Root-Finding, Optimization, and Adjoint Differentiation,mit18_s096iap23_lec06_pdf,lecture,18.s096-iap-2023,5,resources,Resources,1,6,9,,resources/mit18_s096iap23_lec06_pdf,{}
10,Lecture 5, Part 1: Derivative of Matrix Determinant and Inverse,mit18_s096iap23_lec07_pdf,lecture,18.s096-iap-2023,5,resources,Resources,1,7,10,,resources/mit18_s096iap23_lec07_pdf,{}
11,Lecture 5, Part 3: Forward and Reverse-Mode Automatic Differentiation,mit18_s096iap23_lec08_pdf,lecture,18.s096-iap-2023,5,resources,Resources,1,8,11,,resources/mit18_s096iap23_lec08_pdf,{}
12,Lecture 6, Part 1a: Differentiating ODE solutions,mit18_s096iap23_lec09_pdf,lecture,18.s096-iap-2023,5,resources,Resources,1,9,12,,resources/mit18_s096iap23_lec09_pdf,{}
13,Lecture 6, Part 1b: Calculus of Variations,mit18_s096iap23_lec10_pdf,lecture,18.s096-iap-2023,5,resources,Resources,1,10,13,,resources/mit18_s096iap23_lec10_pdf,{}
14,Lecture 7, Part 1: Derivatives of Random Functions,mit18_s096iap23_lec11_pdf,lecture,18.s096-iap-2023,5,resources,Resources,2,11,14,,resources/mit18_s096iap23_lec11_pdf,{}
15,Lecture 7, Part 2: Second Derivatives, Bilinear Maps, and Hessian Matrices,mit18_s096iap23_lec12_pdf,lecture,18.s096-iap-2023,5,resources,Resources,2,12,15,,resources/mit18_s096iap23_lec12_pdf,{}
16,Lecture 8, Part 1: Derivatives of Eigenproblems,mit18_s096iap23_lec13_pdf,lecture,18.s096-iap-2023,5,resources,Resources,2,13,16,,resources/mit18_s096iap23_lec13_pdf,{}
17,Lecture 8, Part 3: Where We Go From Here,mit18_s096iap23_lec14_pdf,lecture,18.s096-iap-2023,5,resources,Resources,2,14,17,,resources/mit18_s096iap23_lec14_pdf,{}
18,Lecture 1 Part 1: Introduction,mit18_s096iap23_lec1_pdf,lecture,18.s096-iap-2023,5,resources,Resources,2,1,18,,resources/mit18_s096iap23_lec1_pdf,{}
19,Lecture 4 Part 2: Nonlinear Root-Finding, Optimization, and Adjoint-Method Differentiation,mit18_s096iap23_lec4_pdf,lecture,18.s096-iap-2023,5,resources,Resources,3,4,19,,resources/mit18_s096iap23_lec4_pdf,{}
20,Lecture 6 Part 1: An Introduction to (Local) Sensitivity Analysis for (Ordinary) Differential Equations,mit18_s096iap23_lec6_pdf,lecture,18.s096-iap-2023,5,resources,Resources,3,6,20,,resources/mit18_s096iap23_lec6_pdf,{}
21,Lecture 7 Part 1: Derivatives of Random Functions,mit18_s096iap23_lec7_pdf,lecture,18.s096-iap-2023,5,resources,Resources,3,7,21,,resources/mit18_s096iap23_lec7_pdf,{}
22,Full Lecture Notes: Matrix Calculus for Machine Learning and Beyond,mit18_s096iap23_lec_full_pdf,lecture,18.s096-iap-2023,5,resources,Resources,3,5,22,,resources/mit18_s096iap23_lec_full_pdf,{}
23,Problem Set 1,mit18_s096iap23_pset1_pdf,problem_set,18.s096-iap-2023,5,resources,Resources,8,5,23,,resources/mit18_s096iap23_pset1_pdf,{}
24,mit18_s096iap23_pset1_t.tex,mit18_s096iap23_pset1_t_tex,problem_set,18.s096-iap-2023,5,resources,Resources,8,5,24,,resources/mit18_s096iap23_pset1_t_tex,{}
25,Problem Set 1 Solutions,mit18_s096iap23_pset1sol_pdf,problem_set,18.s096-iap-2023,5,resources,Resources,8,5,25,,resources/mit18_s096iap23_pset1sol_pdf,{}
26,mit18_s096iap23_pset1sol_t.tex,mit18_s096iap23_pset1sol_t_tex,problem_set,18.s096-iap-2023,5,resources,Resources,8,5,26,,resources/mit18_s096iap23_pset1sol_t_tex,{}
27,Problem Set 2,mit18_s096iap23_pset2_pdf,problem_set,18.s096-iap-2023,5,resources,Resources,9,5,27,,resources/mit18_s096iap23_pset2_pdf,{}
28,mit18_s096iap23_pset2_t.tex,mit18_s096iap23_pset2_t_tex,problem_set,18.s096-iap-2023,5,resources,Resources,9,5,28,,resources/mit18_s096iap23_pset2_t_tex,{}
29,mit18_s096iap23_pset2sol_i.ipynb,mit18_s096iap23_pset2sol_i_ipynb,problem_set,18.s096-iap-2023,5,resources,Resources,9,5,29,,resources/mit18_s096iap23_pset2sol_i_ipynb,{}
30,Problem Set 2 Solutions,mit18_s096iap23_pset2sol_pdf,problem_set,18.s096-iap-2023,5,resources,Resources,9,5,30,,resources/mit18_s096iap23_pset2sol_pdf,{}
31,mit18_s096iap23_pset2sol_t.tex,mit18_s096iap23_pset2sol_t_tex,problem_set,18.s096-iap-2023,5,resources,Resources,9,5,31,,resources/mit18_s096iap23_pset2sol_t_tex,{}
32,Lecture 1 Part 2: Derivatives as Linear Operators,ocw_18s096_lecture01-part2_2023jan18_mp4,lecture,18.s096-iap-2023,5,resources,Resources,3,5,32,,resources/ocw_18s096_lecture01-part2_2023jan18_mp4,{}
33,Lecture 1 Part 1: Introduction and Motivation,ocw_18s096_lecture01_part1_2023jan18_mp4,lecture,18.s096-iap-2023,5,resources,Resources,4,5,33,,resources/ocw_18s096_lecture01_part1_2023jan18_mp4,{}
34,Lecture 2 Part 1: Derivatives in Higher Dimensions: Jacobians and Matrix Functions,ocw_18s096_lecture02-part1_2023jan20_mp4,lecture,18.s096-iap-2023,5,resources,Resources,4,5,34,,resources/ocw_18s096_lecture02-part1_2023jan20_mp4,{}
35,Lecture 2 Part 2: Vectorization of Matrix Functions,ocw_18s096_lecture02-part2_2023jan20_mp4,lecture,18.s096-iap-2023,5,resources,Resources,4,5,35,,resources/ocw_18s096_lecture02-part2_2023jan20_mp4,{}
36,Lecture 3 Part 1: Kronecker Products and Jacobians,ocw_18s096_lecture03-part1_2023jan23_mp4,lecture,18.s096-iap-2023,5,resources,Resources,4,5,36,,resources/ocw_18s096_lecture03-part1_2023jan23_mp4,{}
37,Lecture 3 Part 2: Finite-Difference Approximations,ocw_18s096_lecture03-part2_2023jan23_mp4,lecture,18.s096-iap-2023,5,resources,Resources,4,5,37,,resources/ocw_18s096_lecture03-part2_2023jan23_mp4,{}
38,Lecture 4 Part 1: Gradients and Inner Products in Other Vector Spaces,ocw_18s096_lecture04-part1_2023jan26_mp4,lecture,18.s096-iap-2023,5,resources,Resources,5,5,38,,resources/ocw_18s096_lecture04-part1_2023jan26_mp4,{}
39,Lecture 4 Part 2: Nonlinear Root Finding, Optimization, and Adjoint Gradient Methods,ocw_18s096_lecture04-part2_2023jan26_mp4,lecture,18.s096-iap-2023,5,resources,Resources,5,5,39,,resources/ocw_18s096_lecture04-part2_2023jan26_mp4,{}
40,Lecture 5 Part 1: Derivative of Matrix Determinant and Inverse,ocw_18s096_lecture05-part1-new_2023jan27_mp4,lecture,18.s096-iap-2023,5,resources,Resources,5,5,40,,resources/ocw_18s096_lecture05-part1-new_2023jan27_mp4,{}
41,Lecture 5 Part 2: Forward Automatic Differentiation via Dual Numbers,ocw_18s096_lecture05-part2-new_2023jan27_mp4,lecture,18.s096-iap-2023,5,resources,Resources,5,5,41,,resources/ocw_18s096_lecture05-part2-new_2023jan27_mp4,{}
42,Lecture 5 Part 3: Differentiation on Computational Graphs,ocw_18s096_lecture05-part3-new_2023jan27_mp4,lecture,18.s096-iap-2023,5,resources,Resources,5,5,42,,resources/ocw_18s096_lecture05-part3-new_2023jan27_mp4,{}
43,Lecture 6 Part 1: Adjoint Differentiation of ODE Solutions,ocw_18s096_lecture06-part1_2023jan30_mp4,lecture,18.s096-iap-2023,5,resources,Resources,6,5,43,,resources/ocw_18s096_lecture06-part1_2023jan30_mp4,{}
44,Lecture 6 Part 2: Calculus of Variations and Gradients of Functionals,ocw_18s096_lecture06-part2_2023jan30_mp4,lecture,18.s096-iap-2023,5,resources,Resources,6,5,44,,resources/ocw_18s096_lecture06-part2_2023jan30_mp4,{}
45,Lecture 7 Part 1: Derivatives of Random Functions,ocw_18s096_lecture07-part1_2023feb01_mp4,lecture,18.s096-iap-2023,5,resources,Resources,6,5,45,,resources/ocw_18s096_lecture07-part1_2023feb01_mp4,{}
46,Lecture 7 Part 2: Second Derivatives, Bilinear Forms, and Hessian Matrices,ocw_18s096_lecture07-part2_2023feb01_mp4,lecture,18.s096-iap-2023,5,resources,Resources,6,5,46,,resources/ocw_18s096_lecture07-part2_2023feb01_mp4,{}
47,Lecture 8 Part 1: Derivatives of Eigenproblems,ocw_18s096_lecture08-part1_2023feb03_mp4,lecture,18.s096-iap-2023,5,resources,Resources,6,5,47,,resources/ocw_18s096_lecture08-part1_2023feb03_mp4,{}
48,Lecture 8 Part 2: Automatic Differentiation on Computational Graphs,ocw_18s096_lecture08-part2_2023feb03_mp4,lecture,18.s096-iap-2023,5,resources,Resources,7,5,48,,resources/ocw_18s096_lecture08-part2_2023feb03_mp4,{}
49,18LJMhScijuU1N2nd5TsidHiGolkmnHpA_transcript (notes),resource::static_resources/18LJMhScijuU1N2nd5TsidHiGolkmnHpA_transcript.pdf::seg0,resource,18.s096-iap-2023,5,resources,Resources,10,5,51,,static_resources/18LJMhScijuU1N2nd5TsidHiGolkmnHpA_transcript.pdf,{"snippet": "MITOCW | OCW_18.S096_Lecture08-Part1_2023feb03.mp4\n[SQUEAKING]\n[RUSTLING]\n[CLICKING]\nALAN\nEDELMAN:\nToday, the plan would be I'm going to talk a little bit-- I've got this notebook from last year talking about taking\nthe derivative of such matrix quantities as eigenvalues. And so what's going on here is that we are-- the big story\nis orthogonal matrices may play a role if you have symmetric eigenva", "keywords": ["orthogonal matrix", "symmetric matrix", "orthogonal matrices", "matrix land", "projection matrix"], "resource_label": null}
50,19_-b47PnGFbw50sEzpqpqBycmlTgPmpf_transcript (notes),resource::static_resources/19_-b47PnGFbw50sEzpqpqBycmlTgPmpf_transcript.pdf::seg0,resource,18.s096-iap-2023,5,resources,Resources,10,5,52,,static_resources/19_-b47PnGFbw50sEzpqpqBycmlTgPmpf_transcript.pdf,{"snippet": "MITOCW | OCW_18.S096_Lecture07-Part1_2023feb01.mp4\n[SQUEAKING]\n[RUSTLING]\n[CLICKING]\nPROFESSOR:\nSo today, for the first half of the lecture, we're going to have another guest lecturer, Gaurav Arya, who's actually\nan undergraduate at MIT. But he's done research projects with me, and also research projects with Alan\nEdelman's group, and he's done some really interesting recent work on differentiatin", "keywords": ["notion derivative", "gradient estimator", "random functions", "deterministic function", "derivative but"], "resource_label": null}
51,19fn4Quy6Akz0fP5-RDIHQp0dqMAl8mrH_transcript (notes),resource::static_resources/19fn4Quy6Akz0fP5-RDIHQp0dqMAl8mrH_transcript.pdf::seg0,resource,18.s096-iap-2023,5,resources,Resources,10,5,53,,static_resources/19fn4Quy6Akz0fP5-RDIHQp0dqMAl8mrH_transcript.pdf,{"snippet": "MITOCW | OCW_18.S096_Lecture07-Part2_2023feb01.mp4\n[AUDIO LOGO]\n[MOUSE CLICK]\nSTEVEN\nJOHNSON:\nOK, so let's get started. So as I said, I'm going to do second derivatives, which is just going to be the derivative of\nthe derivative. When you have functions from matrices to matrices or something like that, you have to think a\nlittle bit carefully just to make sure we understand what kind of thing the ", "keywords": ["second derivative", "takes vector", "second derivatives", "vector space", "gradient partial partial"], "resource_label": null}
52,1A065H99Yhsv0hU1C-wFftrck9xfhs31f_transcript (notes),resource::static_resources/1A065H99Yhsv0hU1C-wFftrck9xfhs31f_transcript.pdf::seg0,resource,18.s096-iap-2023,5,resources,Resources,10,5,54,,static_resources/1A065H99Yhsv0hU1C-wFftrck9xfhs31f_transcript.pdf,{"snippet": "MITOCW | OCW_18.S096_Lecture04-Part2_2023jan26.mp4\n[SQUEAKING]\n[RUSTLING]\n[CLICKING]\nSTEVEN G.\nJOHNSON:\nOK. So I think we can get started. So the next part, I'm going to show some slides, so talking about some\napplications. As usual, these slides will be posted later today to the GitHub page. And also the handwritten notes\nalso from the first part will also be posted. They're probably already post", "keywords": ["row vector", "jacobian matrix", "chain rule", "system equations", "nonlinear equations"], "resource_label": null}
53,1BCmBU7TvgZ_F7ShXozkGA0p2m7MKzpyW_transcript (notes),resource::static_resources/1BCmBU7TvgZ_F7ShXozkGA0p2m7MKzpyW_transcript.pdf::seg0,resource,18.s096-iap-2023,5,resources,Resources,10,5,55,,static_resources/1BCmBU7TvgZ_F7ShXozkGA0p2m7MKzpyW_transcript.pdf,{"snippet": "MITOCW | OCW_18.S096_Lecture08-Part2_2023feb03.mp4\n[SQUEAKING]\n[RUSTLING]\n[CLICKING]\nPROFESSOR:\nAnd I just-- since I think it's been over a week, let me just kind of remind you the toy problem that I put up about\na week ago. And the problem was where I was going to input x and y. Maybe you all remember this.\nAnd I had a kind of a program, if you will. I kind of think of this as sort of steps in so", "keywords": ["chain rule", "product rule", "data structure", "power series", "differential geometry"], "resource_label": null}
54,1BZPcIy3JwKPgDcOSM-zp_293D_eKdPn3_transcript (notes),resource::static_resources/1BZPcIy3JwKPgDcOSM-zp_293D_eKdPn3_transcript.pdf::seg0,resource,18.s096-iap-2023,5,resources,Resources,11,5,56,,static_resources/1BZPcIy3JwKPgDcOSM-zp_293D_eKdPn3_transcript.pdf,{"snippet": "MITOCW | OCW_18.S096_Lecture03-Part1_2023jan23.mp4\n[SQUEAKING]\n[RUSTLING]\n[CLICKING]\nALAN\nEDELMAN:\nSo a quick problem. You might want to grab a pen and paper, and-- I think it's not that hard, but let's just ask.\nLet's say we have this function, which is just the 2-norm of x From Rn to R. So everybody knows this function, of\ncourse. It's the square root of x1 squared up to xn squared. So a vector ", "keywords": ["matrix square", "matrix multiply", "matrix cube", "linear function", "matrix cube function"], "resource_label": null}
55,1MUqxaVn3vCENDoyF7G9Dby6e1__MoKed_transcript (notes),resource::static_resources/1MUqxaVn3vCENDoyF7G9Dby6e1__MoKed_transcript.pdf::seg0,resource,18.s096-iap-2023,5,resources,Resources,11,5,57,,static_resources/1MUqxaVn3vCENDoyF7G9Dby6e1__MoKed_transcript.pdf,{"snippet": "MITOCW | OCW_18.S096_Lecture01_Part1_2023jan18.mp4\n[SQUEAKING]\n[RUSTLING]\n[CLICKING]\nALAN\nEDELMAN:\nSo welcome, everybody, to this IAP class on matrix calculus. I'm Professor Alan Edelman. And the other professor\nis Steven Johnson, who's going to have-- he had to go out of town. And so we're going to take advantage of Zoom\ntechnology so that he could give lectures remotely. We did some testing of t", "keywords": ["matrix calculus", "product rule", "m-by-n matrix", "matrix but", "function time"], "resource_label": null}
56,1MaSD7wRIORqNUBKpMSHiFc05ZoQJdz0d_transcript (notes),resource::static_resources/1MaSD7wRIORqNUBKpMSHiFc05ZoQJdz0d_transcript.pdf::seg0,resource,18.s096-iap-2023,5,resources,Resources,11,5,58,,static_resources/1MaSD7wRIORqNUBKpMSHiFc05ZoQJdz0d_transcript.pdf,{"snippet": "MITOCW | OCW_18.S096_Lecture02-Part2_2023jan20.mp4\n[SQUEAKING]\n[RUSTLING]\n[CLICKING]\nSTEVEN\nJOHNSON:\nSo I want to briefly start to talk about going beyond 18.02 derivatives. So we've already gone, I would say,\nbeyond 18.02 in the sense that they never do the-- they have Jacobians maybe, but they never really do the\nchain rule. They never really write even this definition in-- where is it?\nGo back-", "keywords": ["product rule", "by- matrix", "vector spaces", "matrix input", "vector space"], "resource_label": null}
57,1N_XGswpraIiuxsUUuS3yx1Pt82iTV0Nm_transcript (notes),resource::static_resources/1N_XGswpraIiuxsUUuS3yx1Pt82iTV0Nm_transcript.pdf::seg0,resource,18.s096-iap-2023,5,resources,Resources,11,5,59,,static_resources/1N_XGswpraIiuxsUUuS3yx1Pt82iTV0Nm_transcript.pdf,{"snippet": "MITOCW | OCW_18.S096_Lecture06-Part2_2023jan30.mp4\nPROFESSOR:\nThis is a topic that's sometimes called calculus of variations. That's the classic name for it. And-- Cookie, are you\ncausing trouble? OK. Calculus of variations, but all it really is the same idea of derivatives where-- so we're going\nto have f of u, a plus du, minus f of u. That's our df. Is going to be-- I'm going to call it f prime ", "keywords": ["vector space", "column vectors", "takes function", "euler-lagrange equations", "dot products functions"], "resource_label": null}
58,1Oqqwvuk1erjzkpYPF-Xvh94Jw2Xz1fQC_transcript (notes),resource::static_resources/1Oqqwvuk1erjzkpYPF-Xvh94Jw2Xz1fQC_transcript.pdf::seg0,resource,18.s096-iap-2023,5,resources,Resources,11,5,60,,static_resources/1Oqqwvuk1erjzkpYPF-Xvh94Jw2Xz1fQC_transcript.pdf,{"snippet": "MITOCW | OCW_18.S096_Lecture02-Part1_2023jan20.mp4\n[SQUEAKING]\n[RUSTLING]\n[CLICKING]\nALAN\nEDELMAN:\nAll right. Well, welcome to the second lecture for this IAP class on matrix calculus. Steven, do you want to say\nanything before I start on nonlinear versus linear maps? So Philip's going to help out right away. So here's a\npicture of Philip, the corgi. He's right over there, in case you haven't seen", "keywords": ["chain rule", "times matrix", "vector times", "row vector", "vector times matrix"], "resource_label": null}
59,1VwFjcMbHFwFssQo1Zt-V79u0RTzYctzq_transcript (notes),resource::static_resources/1VwFjcMbHFwFssQo1Zt-V79u0RTzYctzq_transcript.pdf::seg0,resource,18.s096-iap-2023,5,resources,Resources,12,5,61,,static_resources/1VwFjcMbHFwFssQo1Zt-V79u0RTzYctzq_transcript.pdf,{"snippet": "MITOCW | OCW_18.S096_Lecture01-Part2_2023jan18.mp4\n[SQUEAKING]\n[RUSTLING]\n[CLICKING]\nSTEVEN G.\nJOHNSON:\nSo I want to revisit the things that Alan talked about, but just a little bit more slowly and a bit more-- just try and\nlay out the rules for you as clearly as I can. And what we're going to try and do is, again, just revisit the notion of\na derivative to try and write it in a way that we can ge", "keywords": ["vector space", "takes vector", "taylor series", "row vector", "scalar function"], "resource_label": null}
60,1_XDieLWJP4p5DE7UXiXwBipmXi0ACNg8_transcript (notes),resource::static_resources/1_XDieLWJP4p5DE7UXiXwBipmXi0ACNg8_transcript.pdf::seg0,resource,18.s096-iap-2023,5,resources,Resources,12,5,62,,static_resources/1_XDieLWJP4p5DE7UXiXwBipmXi0ACNg8_transcript.pdf,{"snippet": "MITOCW | OCW_18.S096_Lecture03-Part2_2023jan23.mp4\n[SQUEAKING]\n[RUSTLING]\n[CLICKING]\nSTEVEN\nJOHNSON:\nSo Alan mentioned fine differences as something that automatic differentiation is not doing. But I do want to talk\nabout fine differences because they do come up a lot when you're talking about computing derivatives on a\ncomputer.\nAnd as he mentioned, we're going to spend a lot of time talking abou", "keywords": ["taylor series", "exact derivative", "second derivative", "rule thumb", "derivative times"], "resource_label": null}
61,1htxMS__1IWJEd7adujbPgMCJIUkYUS3Q_transcript (notes),resource::static_resources/1htxMS__1IWJEd7adujbPgMCJIUkYUS3Q_transcript.pdf::seg0,resource,18.s096-iap-2023,5,resources,Resources,12,5,63,,static_resources/1htxMS__1IWJEd7adujbPgMCJIUkYUS3Q_transcript.pdf,{"snippet": "MITOCW | OCW_18.S096_Lecture06-Part1_2023jan30.mp4\n[SQUEAKING]\n[RUSTLING]\n[CLICKING]\nFRANK\nSCHAFER:\nYeah, so, yeah, I plan to split it in two parts, so the first part is about ordinary differential equations, so just a\nquick recap what an ordinary differential equation actually is, and then we'll go into this kind of sensitivity\nanalysis, so how to compute derivatives of solutions of ordinary diff", "keywords": ["differential equation", "ordinary differential", "ordinary differential equation", "cost function", "differential equations"], "resource_label": null}
62,1m8U5Q3yI56uAUfpAwETeNaNlKVOJM8qp_transcript (notes),resource::static_resources/1m8U5Q3yI56uAUfpAwETeNaNlKVOJM8qp_transcript.pdf::seg0,resource,18.s096-iap-2023,5,resources,Resources,12,5,64,,static_resources/1m8U5Q3yI56uAUfpAwETeNaNlKVOJM8qp_transcript.pdf,{"snippet": "MITOCW | OCW_18.S096_Lecture04-Part1_2023jan26.mp4\n[SQUEAKING]\n[RUSTLING]\n[CLICKING]\nSTEVEN\nJOHNSON:\nSo I wanted to start the day just by finishing up a couple-- comment on a couple of additional things from the last\nlecture. So last lecture, we talked about finite difference approximations. In particular, I talked about the most\nobvious approximation, which is just from the definition of the deri", "keywords": ["vector space", "column vector", "takes vector", "column vectors", "vector spaces"], "resource_label": null}
63,mit18_s096iap23_lec01 (notes),resource::static_resources/mit18_s096iap23_lec01.pdf::seg0,resource,18.s096-iap-2023,5,resources,Resources,12,5,65,,static_resources/mit18_s096iap23_lec01.pdf,{"snippet": "Introduction\nThese notes are based on the class as it was run for the second time in January 2023, taught by Professors Alan\nEdelman and Steven G. Johnson at MIT. The previous version of this course, run in January 2022, can be found\non OCW here.\nBoth Professors Edelman and Johnson use he/him pronouns and are in the Department of Mathematics at MIT;\nProf. Edelman is also a Professor in the MIT Com", "keywords": ["product rule", "matrix calculus", "differential product rule", "scalar vector", "vector matrix"], "resource_label": null}
64,mit18_s096iap23_lec02 (notes),resource::static_resources/mit18_s096iap23_lec02.pdf::seg0,resource,18.s096-iap-2023,5,resources,Resources,13,5,66,,static_resources/mit18_s096iap23_lec02.pdf,{"snippet": "\ud835\udc53\ud835\udc65\n\ud835\udc65 \ud835\udc65 +\ud835\udeff\ud835\udc65\ud835\udc53\ud835\udc65 +\ud835\udeff\ud835\udc65\n\ud835\udeff\ud835\udc65\ud835\udeff\ud835\udc53 slope \ud835\udc53!\ud835\udc65small change\nin \u201coutput\u201d\nsmall change\nin \u201cinput\u201d\nlinear \ttermhigher -order\nterms\u03b4\ud835\udc53= \ud835\udc53\ud835\udc65 +\ud835\udeff\ud835\udc65\u2212 \ud835\udc53\ud835\udc65 = \ud835\udc53!\ud835\udc65\ud835\udeff\ud835\udc65\t+ \t \ud835\udc5c( \ud835\udeff\ud835\udc65)Figure 1: The essence of a derivative is linearization : predicting a small change \u03b4fin the output f(x)from a small\nchange \u03b4xin the input x, tofirst order in\u03b4x.\n2 Derivatives as Linear Operators\nWe are now going to revisit the notion of a derivative in a w", "keywords": ["chain rule", "vector spaces", "vector space", "column vector", "directional derivative"], "resource_label": null}
65,mit18_s096iap23_lec03 (notes),resource::static_resources/mit18_s096iap23_lec03.pdf::seg0,resource,18.s096-iap-2023,5,resources,Resources,13,5,67,,static_resources/mit18_s096iap23_lec03.pdf,{"snippet": "3 Jacobians of Matrix Functions\nWhen we have a function that has matrices as inputs and/or outputs, we have already seen in the previous lectures\nthat we can still define the derivative as a linear operator by a formula forf\u2032mapping a small change in input to\nthe corresponding small change in output. However, when you first learned linear algebra, probably most linear\noperationswererepresentedbyma", "keywords": ["matrix functions", "matrix-square function", "column vectors", "partial derivatives", "matrix inputs"], "resource_label": null}
66,mit18_s096iap23_lec04 (notes),resource::static_resources/mit18_s096iap23_lec04.pdf::seg0,resource,18.s096-iap-2023,5,resources,Resources,13,5,68,,static_resources/mit18_s096iap23_lec04.pdf,{"snippet": "4 Finite-Difference Approximations\nIn this section, we will be referring to this Julia notebook for calculations that are not included here.\n4.1 Why compute derivatives approximately instead of exactly?\nWorking out derivatives by hand is a notoriously error-prone procedure for complicated functions. Even if every\nindividual step is straightforward, there are so many opportunities to make a mistake", "keywords": ["analytical derivative", "analytical derivatives", "product rule", "why compute derivatives", "compute derivatives approximately"], "resource_label": null}
67,mit18_s096iap23_lec05 (notes),resource::static_resources/mit18_s096iap23_lec05.pdf::seg0,resource,18.s096-iap-2023,5,resources,Resources,13,5,69,,static_resources/mit18_s096iap23_lec05.pdf,{"snippet": "5 Derivatives in General Vector Spaces\nMatrix calculus requires us to generalize concepts of derivative and gradient further, to functions whose inputs\nand/or outputs are not simply scalars or column vectors. To achieve this, we extend the notion of the ordinary\nvectordot product and ordinary Euclidean vector \u201clength\u201d to general inner products andnormsonvector\nspaces. Our first example will consid", "keywords": ["vector space", "vector spaces", "column vectors", "complex vector spaces", "space complete vector"], "resource_label": null}
68,mit18_s096iap23_lec06 (notes),resource::static_resources/mit18_s096iap23_lec06.pdf::seg0,resource,18.s096-iap-2023,5,resources,Resources,13,5,70,,static_resources/mit18_s096iap23_lec06.pdf,{"snippet": "6 Nonlinear Root-Finding, Optimization,\nand Adjoint Differentiation\nThe next part is based on these slides. Today, we want to talk about why we are computing derivatives in the first\nplace. In particular, we will drill down on this a little bit and then talk about computation of derivatives.\n6.1 Newton\u2019s Method\nOne common application of derivatives is to solve nonlinear equations via linearization", "keywords": ["chain rule", "nonlinear equations", "scalar-valued function", "differential matrix inverse", "scalar function"], "resource_label": null}
69,mit18_s096iap23_lec07 (notes),resource::static_resources/mit18_s096iap23_lec07.pdf::seg0,resource,18.s096-iap-2023,5,resources,Resources,14,5,71,,static_resources/mit18_s096iap23_lec07.pdf,{"snippet": "7 Derivative of Matrix Determinant and Inverse\n7.1 Two Derivations\nThis section of notes follows thisJulia notebook. This notebook is a little bit short, but is an important and useful\ncalculation.\nTheorem 39\nGiven Ais a square matrix, we have\n\u2207(det A) = cofactor( A) = (det A)A\u2212T:= adj( AT) = adj( A)T\nwhere adjis the \u201cadjugate\u201d. (You may not have heard of the matrix adjugate, but this formula tell", "keywords": ["explicit jacobian matrix", "jacobian matrix", "cofactor matrix", "logarithmic derivative", "derivative matrix determinant"], "resource_label": null}
70,mit18_s096iap23_lec08 (notes),resource::static_resources/mit18_s096iap23_lec08.pdf::seg0,resource,18.s096-iap-2023,5,resources,Resources,14,5,72,,static_resources/mit18_s096iap23_lec08.pdf,{"snippet": "8 Forward and Reverse-Mode Automatic Differentiation\nThe first time that Professor Edelman had heard about automatic differentiation (AD), it was easy for him to\nimagine what it was ...but what he imagined was wrong! In his head, he thought it was straightforward symbolic\ndifferentiation applied to code\u2014sort of like executing Mathematica or Maple, or even just automatically doing what\nhe learned t", "keywords": ["chain rule", "hessian vector", "babylonian algorithm", "scalar-valued function", "hessian vector product"], "resource_label": null}
71,mit18_s096iap23_lec09 (notes),resource::static_resources/mit18_s096iap23_lec09.pdf::seg0,resource,18.s096-iap-2023,5,resources,Resources,14,5,73,,static_resources/mit18_s096iap23_lec09.pdf,{"snippet": "9 Differentiating ODE solutions\nIn this lecture, we will consider the problem of differentiating the solution of ordinary differential equations (ODEs)\nwith respect to parameters that appear in the equations and/or initial conditions. This is as important topic in a\nsurprising number of practical applications, such as evaluating the effect of uncertainties, fitting experimental data,\nor machine le", "keywords": ["experimental data", "fitting experimental data", "ordinary differential", "differential equations", "scalar function"], "resource_label": null}
72,mit18_s096iap23_lec1 (notes),resource::static_resources/mit18_s096iap23_lec1.pdf::seg0,resource,18.s096-iap-2023,5,resources,Resources,14,5,74,,static_resources/mit18_s096iap23_lec1.pdf,{"snippet": "Introduction and Syllabus for 18.S096: \nMatrix Calculus \nIAP 2023 \nProfs. Alan Edelman & Steven Johnson \nMWF 11am\u20131pm in 2-190 \n(MIT students, comments, fixes, request for further clarifications, welcome.  Please keep mathematical) 1\n\u25cf Lectures: Jan. 18,20,23,25,27 + Feb. 1,3 \n\u25cb 11am\u20131pm in 2-142, short break around noon \n\u25cf\n Two Psets:  Released Wednesday due following \nW\nednesday (Jan 19 & 26) @ ", "keywords": ["vector matrix", "matrix calculus", "scalar vector", "matrix higher", "matrix matrix"], "resource_label": null}
73,mit18_s096iap23_lec10 (notes),resource::static_resources/mit18_s096iap23_lec10.pdf::seg0,resource,18.s096-iap-2023,5,resources,Resources,14,5,75,,static_resources/mit18_s096iap23_lec10.pdf,{"snippet": "10 Calculus of Variations\nIn this lecture, we will apply our derivative machinery to a new type of input: neither scalars, nor column vectors,\nnor matrices, but rather the inputs will be functions u(x), which form a perfectly good vector space (and can\neven have norms and inner products).12It turns out that there are lots of amazing applications for differentiating\nwith respect to functions, and t", "keywords": ["inner products functions", "euler lagrange equations", "products functions", "differentiable function", "lagrange equations"], "resource_label": null}
74,mit18_s096iap23_lec11 (notes),resource::static_resources/mit18_s096iap23_lec11.pdf::seg0,resource,18.s096-iap-2023,5,resources,Resources,15,5,76,,static_resources/mit18_s096iap23_lec11.pdf,{"snippet": "11 Derivatives of Random Functions\nThese notes are from a guest lecture by Gaurav Arya in IAP 2023.\n11.1 Introduction\nIn this class, we\u2019ve learned how to take derivatives of all sorts of crazy functions. Recall one of our first examples:\nf(A) = A2, (8)\nwhere Ais a matrix. To differentiate this function, we had to go back to the drawing board, and ask:\nQuestion 50. If we perturb the input slightly,", "keywords": ["sample generic function", "generic function", "notion derivative", "useful notion derivative", "random functions"], "resource_label": null}
75,mit18_s096iap23_lec11 (notes),resource::static_resources/mit18_s096iap23_lec11.pdf::seg1,resource,18.s096-iap-2023,5,resources,Resources,15,5,77,,static_resources/mit18_s096iap23_lec11.pdf,{"snippet": "0\nX(p+\u000ep)\nX(p)\n\u000eX(\np)\nO(\u000ep)\n!1\n!2\n1\nf\nf\nNot\nO(\u000ep)Figure 13: For X(p)\u223cExp(p)parameterized via the inversion method, we can write X(p),X(p+\u03b4p), and \u03b4X(p)\nas functions from \u2126 = [0, 1]\u2192R, defined on a probability space with P= Unif(0, 1).\nfunctions over \u2126. To sample the two of them jointly, we use the samechoice of \u03c9: thus, \u03b4X(p)can be formed by\nsubtracting the two functions pointwise at each \u2126. Ultim", "keywords": ["generic function", "gradient estimator", "sample generic function", "notion derivative", "chain rule"], "resource_label": null}
76,mit18_s096iap23_lec11 (notes),resource::static_resources/mit18_s096iap23_lec11.pdf::seg2,resource,18.s096-iap-2023,5,resources,Resources,15,5,78,,static_resources/mit18_s096iap23_lec11.pdf,{"snippet": "0\n1\u0000p\u0000\u000ep\n1\u0000p\nX(p+\u000ep)\nX(p)\n1\n1\n\u000eX(\np)\n1\u0000p\u0000\u000ep\n1\u0000p\nOutput", "keywords": ["output"], "resource_label": null}
77,mit18_s096iap23_lec11 (notes),resource::static_resources/mit18_s096iap23_lec11.pdf::seg3,resource,18.s096-iap-2023,5,resources,Resources,15,5,79,,static_resources/mit18_s096iap23_lec11.pdf,{"snippet": "0\nX(p)(! ) =\n0\nX(p)(! ) =\n1\nf\nf\n1\n1Figure 14: For X(p)\u223cBer(p)parameterized via the inversion method, plots of X(p),X(p+\u03b4p), and \u03b4X(p)as\nfunctions \u2126 : [0 ,1]\u2192R.\njulia> sample_X(\u03b4p)\nfalse\njulia> sample_X(\u03b4p)\ntrue\nThe parameterization of a Bernoulli variable is shown in Figure 2. Using the inversion method once again, the\nparameterization of a Bernoulli variable looks like a step function: for \u03c9 <1\u2212p", "keywords": ["derivative contribution", "functions julia sample", "like step function", "defined almost-sure limit", "certainly important derivative"], "resource_label": null}
78,mit18_s096iap23_lec12 (notes),resource::static_resources/mit18_s096iap23_lec12.pdf::seg0,resource,18.s096-iap-2023,5,resources,Resources,15,5,80,,static_resources/mit18_s096iap23_lec12.pdf,{"snippet": "12 Second Derivatives, Bilinear Maps,\nand Hessian Matrices\nIn this chapter, we apply the principles of this course to secondderivatives, which are conceptually just derivatives\nof derivatives but turn out to have many interesting ramifications. We begin with a (probably) familiar case of\nscalar-valued functions from multi-variable calculus, in which the second derivative is simply a matrix called ", "keywords": ["hessian matrix", "second derivative", "second derivatives", "hessian matrices", "scalar-valued functions"], "resource_label": null}
79,mit18_s096iap23_lec13 (notes),resource::static_resources/mit18_s096iap23_lec13.pdf::seg0,resource,18.s096-iap-2023,5,resources,Resources,16,5,81,,static_resources/mit18_s096iap23_lec13.pdf,{"snippet": "13 Derivatives of Eigenproblems\n13.1 Differentiating on the Unit Sphere\nGeometrically, we know that velocity vectors (equivalently, tangents) on the sphere are orthogonal to the radii.\nOut differentials say this algebraically, since given x\u2208Snwe have xTx= 1, this implies that\n2xTdx=d(xTx) = d(1) = 0 .\nIn other words, at the point xon the sphere (a radius, if you will), dx, the linearization of the", "keywords": ["orthogonal matrices", "symmetric matrix", "theorem given", "orthogonal matrix", "derivatives eigenproblems differentiating"], "resource_label": null}
80,mit18_s096iap23_lec14 (notes),resource::static_resources/mit18_s096iap23_lec14.pdf::seg0,resource,18.s096-iap-2023,5,resources,Resources,16,5,82,,static_resources/mit18_s096iap23_lec14.pdf,{"snippet": "14 Where We Go From Here\nThere are many topics that we did not have time to cover, even in 16 hours of lectures. If you came into this class\nthinking that taking derivatives is easy and you already learned everything there is to know about it in first-year\ncalculus, hopefully we\u2019ve convinced you that it is an enormously rich subject that is impossible to exhaust in a\nsingle course. Some of the thi", "keywords": ["taking derivatives easy", "custom jacobian vector", "jacobian vector product", "vector product jvp", "custon row vector"], "resource_label": null}
81,mit18_s096iap23_lec4 (notes),resource::static_resources/mit18_s096iap23_lec4.pdf::seg0,resource,18.s096-iap-2023,5,resources,Resources,16,5,83,,static_resources/mit18_s096iap23_lec4.pdf,{"snippet": "Matrix Calculus  lecture notes: \nHow can we use so many derivatives ? \n\u2026 a couple of applications \n\u2026 and the \u201cadjoint method\u201d \nMatrix Calculus , IAP 2023 \nProfs. Steven G. Johnson & Alan Edelman, MIT \n1\nNewton\u2019s method: Nonlinear equations via Linearization \n18.01: solving f(x) = 0 : \n1. Linearize: \n           f(x+\u03b4x) \u2248 f(x) + f\u2032(x)\u03b4x \n2. Solve linear equation \nf(x) + f\u2019(x)\u03b4x = 0 \n\u21d2 \u03b4x = \u2013f(x)/f\u2032(", "keywords": ["matrix calculus", "solve linear equation", "linear equation update", "example gradient scalar", "then all derivatives"], "resource_label": null}
82,mit18_s096iap23_lec6 (notes),resource::static_resources/mit18_s096iap23_lec6.pdf::seg0,resource,18.s096-iap-2023,5,resources,Resources,16,5,84,,static_resources/mit18_s096iap23_lec6.pdf,{"snippet": "An Introduction to (Local) Sensitivity Analysis \nfor (Ordinary) Differential Equations\n18.S096 Special Subject:\nMatrix Calculus for Machine Learning and Beyond by Professors AlanEdelman  and Steven G. Johnson\n1Julia Lab, CSAIL, MITFrank Sch\u00e4fer1\n1\nAn Introduction to (Local) Sensitivity Analysis \nfor (Ordinary) Differential Equations\n18.S096 Special Subject:\nMatrix Calculus for Machine Learning and", "keywords": ["ordinary differential equations", "ordinary differential", "differential equations", "exact gradient", "matrix calculus"], "resource_label": null}
83,mit18_s096iap23_lec7 (notes),resource::static_resources/mit18_s096iap23_lec7.pdf::seg0,resource,18.s096-iap-2023,5,resources,Resources,16,5,85,,static_resources/mit18_s096iap23_lec7.pdf,{"snippet": "Derivatives of Random Functions \n(MIT 18.S096, Lecture 7) \nGaurav Arya \nFebruary 1, 2023 \n1 Introduction \nIn this class, we\u2019ve learned how to take derivatives of all sorts of crazy functions. Recall \none of our first examples: \nf(A) = A 2 , (1) \nwhere A is a matrix. To di\u02d9erentiate this function, we had to go back to the drawing \nboard, and ask: \nQ1: If we perturb the input slightly, how does the ", "keywords": ["generic function", "notion derivative", "sample generic function", "gradient estimator", "chain rule"], "resource_label": null}
84,mit18_s096iap23_lec_full (notes),resource::static_resources/mit18_s096iap23_lec_full.pdf::seg0,resource,18.s096-iap-2023,5,resources,Resources,17,5,86,,static_resources/mit18_s096iap23_lec_full.pdf,{"snippet": "Matrix Calculus\n(for Machine Learning and Beyond)\nLecturers: Alan Edelman and Steven G. Johnson\nNotes by Paige Bright, Alan Edelman, and Steven G. Johnson\nBased on MIT course 18.S096 (now 18.063) in IAP 2023\nContents\nIntroduction 4\n1 Overview and Motivation 5\n1.1 Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\n1.2 First Derivatives", "keywords": ["chain rule", "vector spaces", "vector space", "jacobian matrix", "product rule"], "resource_label": null}
85,mit18_s096iap23_lec_full (notes),resource::static_resources/mit18_s096iap23_lec_full.pdf::seg1,resource,18.s096-iap-2023,5,resources,Resources,17,5,87,,static_resources/mit18_s096iap23_lec_full.pdf,{"snippet": "0\nX(p+\u000e\np)\nX(p)\n\u000e\nX(p)\nO(\u000e\np)\n!1\n!2\n1\nf\nf\nNot\nO(\u000e\np)Figure 13: For X(p)\u223cExp(p)parameterized via the inversion method, we can write X(p),X(p+\u03b4p), and \u03b4X(p)\nas functions from \u2126 = [0, 1]\u2192R, defined on a probability space with P= Unif(0, 1).\nfunctions over \u2126. To sample the two of them jointly, we use the samechoice of \u03c9: thus, \u03b4X(p)can be formed by\nsubtracting the two functions pointwise at each \u2126. Ul", "keywords": ["generic function", "gradient estimator", "sample generic function", "notion derivative", "chain rule"], "resource_label": null}
86,mit18_s096iap23_lec_full (notes),resource::static_resources/mit18_s096iap23_lec_full.pdf::seg2,resource,18.s096-iap-2023,5,resources,Resources,17,5,88,,static_resources/mit18_s096iap23_lec_full.pdf,{"snippet": "0\n1\u0000p\u0000\u000e\np\n1\u0000p\nX(p+\u000e\np)\nX(p)\n1\n1\n\u000e\nX(p)\n1\u0000p\u0000\u000e\np\n1\u0000p\nOutput", "keywords": ["output"], "resource_label": null}
87,mit18_s096iap23_lec_full (notes),resource::static_resources/mit18_s096iap23_lec_full.pdf::seg3,resource,18.s096-iap-2023,5,resources,Resources,17,5,89,,static_resources/mit18_s096iap23_lec_full.pdf,{"snippet": "0\nX(p)(!)\n= 0\nX(p)(!)\n= 1\nf\nf\n1\n1Figure\n14: For X(p)\u223cBer(p)parameterized via the inversion method, plots of X(p),X(p+\u03b4p), and \u03b4X(p)as\nfunctions \u2126 : [0 ,1]\u2192R.\njulia> sample_X(\u03b4p)\nfalse\njulia> sample_X(\u03b4p)\ntrue\nThe parameterization of a Bernoulli variable is shown in Figure 2. Using the inversion method once again, the\nparameterization of a Bernoulli variable looks like a step function: for \u03c9 <1\u2212p,X", "keywords": ["hessian matrix", "second derivative", "second derivatives", "hessian matrices", "symmetric matrix"], "resource_label": null}
88,mit18_s096iap23_pset1 (notes),resource::static_resources/mit18_s096iap23_pset1.pdf::seg0,resource,18.s096-iap-2023,5,resources,Resources,17,5,90,,static_resources/mit18_s096iap23_pset1.pdf,{"snippet": "18.S096 PSET 1\nIAP 2023\nProblem 0 (4+4+4+4 points)\nThe hyperbolic Corgi notebook may be found at https://mit-c25.netlify.app/notebooks/1_hyperbolic_corgi. Com-\npute the 2\u00d72Jacobian matrix for each of the following image transformations from that notebook:\n(a) rotate( \u03b8):(x, y)\u2192(cos(\u03b8 )x+ sin(\u03b8 )y,\u2212sin(\u03b8)x+ cos(\u03b8 )y)\n(b) hyperbolic_rotate( \u03b8):(x, y)\u2192(cosh(\u03b8 )x+ sinh(\u03b8 )y,sinh(\u03b8 )x+ cosh(\u03b8 )y)\n(c) n", "keywords": ["vector space", "jacobian matrix", "linear functions", "give ajacobian matrix", "vector inputs rnand"], "resource_label": null}
89,mit18_s096iap23_pset1sol (notes),resource::static_resources/mit18_s096iap23_pset1sol.pdf::seg0,resource,18.s096-iap-2023,5,resources,Resources,18,5,91,,static_resources/mit18_s096iap23_pset1sol.pdf,{"snippet": "18.S096 PSET 1 Solutions\nIAP 2023\nFebruary 3, 2023\nProblem 0 (4+4+4+4 points)\nThe hyperbolic Corgi notebook may be found at https://mit-c25.netlify.app/notebooks/1_hyperbolic_corgi. Com-\npute the 2\u00d72Jacobian matrix for each of the following image transformations from that notebook:\n(a) rotate( \u03b8):(x, y)\u2192(cos(\u03b8 )x+ sin(\u03b8 )y,\u2212sin(\u03b8)x+ cos(\u03b8 )y)\nSolution: This is simply a linear function from R2\u2192R2\n ", "keywords": ["vector space", "jacobian matrix", "product rule", "linear functions", "chain rule"], "resource_label": null}
90,mit18_s096iap23_pset2 (notes),resource::static_resources/mit18_s096iap23_pset2.pdf::seg0,resource,18.s096-iap-2023,5,resources,Resources,18,5,92,,static_resources/mit18_s096iap23_pset2.pdf,{"snippet": "18.S096 PSET 2, IAP 2023\nProblem 1 (5+5+5 points)\nSuppose that A(p)takes a vector p\u2208Rn\u22121and returns the n\u00d7ntridiagonal real-symmetric matrix\nA(p) =\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8eda1p1\np1a2p2\np2......\n...an\u22121 pn\u22121\npn\u22121 an\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8,\nwhere a\u2208Rn\u22121is some constant vector. Now, define a scalar-valued function f(p)by\nf(p) =\u0000\ncTA(p)\u22121b\u00012\nfor some constant vectors b, c\u2208Rn(assuming we choose pandaso that Ais invertible). Note", "keywords": ["jacobian determinant", "jacobian matrix", "scalar function", "hellman feynman theorem", "jacobian determinant sometimes"], "resource_label": null}
91,mit18_s096iap23_pset2sol (notes),resource::static_resources/mit18_s096iap23_pset2sol.pdf::seg0,resource,18.s096-iap-2023,5,resources,Resources,18,5,93,,static_resources/mit18_s096iap23_pset2sol.pdf,{"snippet": "18.S096 Pset 2 SOLUTIONS, IAP 2023\nProblem 1 (5+5+5 points)\nSuppose that A(p)takes a vector p\u2208Rn\u22121and returns the n\u00d7ntridiagonal real-symmetric matrix\nA(p) =\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8eda1p1\np1a2p2\np2......\n...an\u22121 pn\u22121\npn\u22121 an\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8,\nwhere a\u2208Rn\u22121is some constant vector. Now, define a scalar-valued function f(p)by\nf(p) =\u0000\ncTA(p)\u22121b\u00012\nfor some constant vectors b, c\u2208Rn(assuming we choose pandaso that Ais inverti", "keywords": ["jacobian determinant", "jacobian matrix", "product rule", "scalar function", "hellman feynman theorem"], "resource_label": null}
92,ocw_18s096_lecture05-part1-new_2023jan27_transcript (notes),resource::static_resources/ocw_18s096_lecture05-part1-new_2023jan27_transcript.pdf::seg0,resource,18.s096-iap-2023,5,resources,Resources,18,5,94,,static_resources/ocw_18s096_lecture05-part1-new_2023jan27_transcript.pdf,{"snippet": "Page\t1/23/Users/jcplayer/Desktop/18.S096/OCW_18.S096_Lecture05-Part1-New_2023jan27.txtSaved:\t11/28/23,\t4:15:58\tPMPrinted\tfor:\tJason\tPlayer[SQUEAKING]\u00ac1[RUSTLING]\u00ac2[CLICKING]\u00ac3STEVEN\tG.\tJOHNSON:\u00ac4OK,\tso\tlast\ttime\u00ac5I\ttalked\tabout\thow\tin\u00ac6order\tto\tdefine\ta\tgradient,\u00ac7you\tneed\tan\tinner\tproduct.\u00ac8So\tthat\tway,\tif\tyou\thave\ta\u00ac9scalar\tfunction\tof\ta\tvector,\u00ac10the\tgradient\tis\tdefined--\u00ac11basically\tthe\u00ac12deri", "keywords": ["determinant plus", "vector space", "column vector", "cofactor matrix", "by- matrix"], "resource_label": null}
93,ocw_18s096_lecture05-part2-new_2023jan27_transcript (notes),resource::static_resources/ocw_18s096_lecture05-part2-new_2023jan27_transcript.pdf::seg0,resource,18.s096-iap-2023,5,resources,Resources,18,5,95,,static_resources/ocw_18s096_lecture05-part2-new_2023jan27_transcript.pdf,{"snippet": "Page\t1/28/Users/jcplayer/Desktop/18.S096/OCW_18.S096_Lecture05-Part2-New_2023jan27.txtSaved:\t11/28/23,\t4:15:21\tPMPrinted\tfor:\tJason\tPlayer[SQUEAKING]\u00ac1[RUSTLING]\u00ac2[CLICKING]\u00ac3OK.\u00ac4So\tjust\tto\tlet\tyou\tknow\twhat\u00ac5this\tnotebook\tis\tand\tisn't,\u00ac6this\tnotebook\tis\u00ac7kind\tof\tmeant\tto\tlet\u00ac8you\tsee\thow\tautomatic\u00ac9differentiation\tis\u00ac10kind\tof\tmagical\tin\ta\tway.\u00ac11That's\tkind\tof\tthe\treal\tpurpose.\u00ac12You'll\tstart\tt", "keywords": ["babylonian algorithm", "quotient rule", "derivative square root", "derivative square", "function derivative pair"], "resource_label": null}
94,ocw_18s096_lecture05-part3-new_2023jan27_transcript (notes),resource::static_resources/ocw_18s096_lecture05-part3-new_2023jan27_transcript.pdf::seg0,resource,18.s096-iap-2023,5,resources,Resources,19,5,96,,static_resources/ocw_18s096_lecture05-part3-new_2023jan27_transcript.pdf,{"snippet": "Page\t1/21/Users/jcplayer/Desktop/18.S096/OCW_18.S096_Lecture05-Part3-New_2023jan27.txtSaved:\t11/28/23,\t4:13:32\tPMPrinted\tfor:\tJason\tPlayer[SQUEAKING]\u00ac1[RUSTLING]\u00ac2[CLICKING]\u00ac3PROFESSOR:\tSo\tyou've\u00ac4already\tseen\ta\tlittle\tbit\u00ac5of\tthe\tstory\tof\tforward\u00ac6mode\tand\treverse\u00ac7mode\tfrom\tStephen\tlast\tweek.\u00ac8One\tversion\tof\tthe\u00ac9story\tis\tthat\tyou're\u00ac10multiplying\tderivatives,\u00ac11or\tJacobian\tmatrices,\u00ac12or\tsometh", "keywords": ["one-step derivative", "need data structure", "been determinant", "derivative would", "need data"], "resource_label": null}
95,symeig.jl,symeig_jl,concept,18.s096-iap-2023,5,resources,Resources,8,5,49,,resources/symeig_jl,{}
96,Lecture Videos,lecture-videos,lecture,18.s096-iap-2023,6,video_galleries,Video_Galleries,0,6,50,,video_galleries/lecture-videos,{}
