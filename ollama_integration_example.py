"""
Practical Example: Conversation Health Tracker with Ollama (Local LLM)

This shows how to use the tracker with Ollama or any other local LLM.
Both the LLM and the health tracker run 100% locally.

Requirements:
  pip install sentence-transformers
  # Install Ollama from https://ollama.ai
  # Pull a model: ollama pull llama3.2
"""

import sys
if sys.platform == 'win32':
    sys.stdout.reconfigure(encoding='utf-8')

from conversation_health_tracker import ConversationHealthTracker


class LocalLLMWithHealthTracking:
    """
    Wrapper for local LLM (Ollama, LMStudio, etc.) with conversation health monitoring.
    """

    def __init__(self, model_name: str = "llama3.2", use_ollama: bool = True):
        """
        Initialize LLM and health tracker.

        Args:
            model_name: Ollama model name (e.g., "llama3.2", "mistral", "codellama")
            use_ollama: If True, uses Ollama. If False, simulates responses for demo.
        """
        self.model_name = model_name
        self.use_ollama = use_ollama
        self.tracker = ConversationHealthTracker()
        self.conversation_history = []

        # Try to import ollama
        if use_ollama:
            try:
                import ollama
                self.ollama = ollama
                print(f"‚úì Connected to Ollama (model: {model_name})")
            except ImportError:
                print("‚ö† Ollama not installed. Install with: pip install ollama")
                print("  Or run demo mode by setting use_ollama=False")
                self.use_ollama = False

    def chat(self, user_message: str, show_health: bool = True) -> dict:
        """
        Send message to LLM and analyze conversation health.

        Args:
            user_message: User's message
            show_health: Whether to print health metrics

        Returns:
            dict with 'response', 'health', 'guidance'
        """
        print(f"\n{'='*80}")
        print(f"üë§ USER: {user_message}")
        print(f"{'='*80}\n")

        # Get system prompt (includes health guidance if available)
        system_prompt = self._build_system_prompt()

        # Get response from LLM
        if self.use_ollama:
            assistant_message = self._query_ollama(user_message, system_prompt)
        else:
            assistant_message = self._simulate_response(user_message)

        print(f"ü§ñ ASSISTANT: {assistant_message}\n")

        # Analyze conversation health
        self.tracker.add_turn('user', user_message)
        health = self.tracker.add_turn('assistant', assistant_message)

        # Generate guidance if needed
        guidance = None
        if health.get('alerts'):
            guidance = self.tracker.generate_coaching_summary()

        # Store in history
        self.conversation_history.append({
            'user': user_message,
            'assistant': assistant_message,
            'health': health
        })

        # Display health metrics
        if show_health:
            self._print_health_metrics(health, guidance)

        return {
            'response': assistant_message,
            'health': health,
            'guidance': guidance
        }

    def _build_system_prompt(self) -> str:
        """Build system prompt with conversation health guidance if needed."""
        base_prompt = """You are a helpful AI assistant. Be direct and answer questions specifically."""

        # Get recent health summary
        if len(self.conversation_history) > 0:
            last_health = self.conversation_history[-1]['health']
            if last_health.get('alerts'):
                guidance = self.tracker.generate_coaching_summary()
                return f"{base_prompt}\n\n[CONVERSATION HEALTH FEEDBACK]\n{guidance}"

        return base_prompt

    def _query_ollama(self, user_message: str, system_prompt: str) -> str:
        """Query Ollama API."""
        try:
            response = self.ollama.chat(
                model=self.model_name,
                messages=[
                    {'role': 'system', 'content': system_prompt},
                    {'role': 'user', 'content': user_message}
                ]
            )
            return response['message']['content']
        except Exception as e:
            print(f"‚ö† Ollama error: {e}")
            return "Error querying Ollama. Is the service running? Try: ollama serve"

    def _simulate_response(self, user_message: str) -> str:
        """Simulate LLM response for demo (when Ollama not available)."""
        # Simple rule-based responses for demo
        if "why" in user_message.lower() and "fail" in user_message.lower():
            if "deployment" in user_message.lower():
                return "Deployments can fail for many reasons. Let me explain the deployment process..."
            elif "test" in user_message.lower():
                return "Testing is complex. There are many types of tests: unit tests, integration tests..."
            else:
                return "There are many possible reasons for failures. Let me explain the general concepts..."
        else:
            return f"I understand you're asking about: {user_message}. Let me provide some general information on this topic..."

    def _print_health_metrics(self, health: dict, guidance: str = None):
        """Print conversation health metrics."""
        print(f"{'‚îÄ'*80}")
        print(f"üìä CONVERSATION HEALTH")
        print(f"{'‚îÄ'*80}")
        print(f"Status: {health['status']}")
        print(f"Context: {health['context']}")

        if 'drift_on_specifics' in health:
            drift = health['drift_on_specifics']
            threshold = self.tracker.DRIFT_THRESHOLDS[health['context']]
            status = "‚ö†Ô∏è ALERT" if drift > threshold else "‚úì OK"
            print(f"Question‚ÜíResponse Drift: {drift:.3f} / {threshold:.3f} {status}")

        if 'topic_coherence' in health:
            print(f"Topic Coherence: {health['topic_coherence']:.3f}")

        if health.get('fragmented'):
            print(f"Fragmentation: ‚ö†Ô∏è YES")

        if health.get('alerts'):
            print(f"\n‚ö†Ô∏è  ALERTS:")
            for alert in health['alerts']:
                print(f"   ‚Ä¢ {alert}")

        if guidance:
            print(f"\nüí° GUIDANCE FOR NEXT TURN:")
            print(f"   (This will be injected into system prompt)")
            for line in guidance.split('\n')[:5]:  # Show first 5 lines
                print(f"   {line}")
            print(f"   [... see full guidance in logs]")

        print(f"{'‚îÄ'*80}\n")


def demo_evasion_pattern():
    """Demonstrate evasion detection with local LLM."""
    print("\n" + "="*80)
    print("DEMO 1: Evasion Pattern Detection")
    print("="*80)
    print("Scenario: User asks specific question, assistant evades")
    print()

    llm = LocalLLMWithHealthTracking(use_ollama=False)  # Demo mode

    # Turn 1: User asks specific question
    result1 = llm.chat("Why did our deployment fail yesterday?")

    # The assistant's evasive response should trigger drift alert
    if result1['guidance']:
        print("\n‚úì Evasion detected! Guidance will be injected into next prompt.\n")

    # Turn 2: User repeats question (now with guidance in system prompt)
    result2 = llm.chat("I'm asking specifically: WHY did the deployment fail?")


def demo_healthy_conversation():
    """Demonstrate healthy conversation pattern."""
    print("\n" + "="*80)
    print("DEMO 2: Healthy Direct Response")
    print("="*80)
    print("Scenario: User asks question, assistant answers directly")
    print()

    llm = LocalLLMWithHealthTracking(use_ollama=False)

    # Manually override simulation to show direct response
    original_simulate = llm._simulate_response

    def direct_response(user_message):
        if "deployment fail" in user_message.lower():
            return "Deployment failed due to Docker image build error on line 23 of Dockerfile. The base image 'node:14' is deprecated. Fix: Update to 'node:18' or later."
        return original_simulate(user_message)

    llm._simulate_response = direct_response

    result = llm.chat("Why did our deployment fail yesterday?")

    if not result['guidance']:
        print("\n‚úì Healthy conversation - no alerts triggered!\n")


def demo_with_real_ollama():
    """Demonstrate with real Ollama (if available)."""
    print("\n" + "="*80)
    print("DEMO 3: Real Ollama Integration")
    print("="*80)

    llm = LocalLLMWithHealthTracking(model_name="llama3.2", use_ollama=True)

    if not llm.use_ollama:
        print("Ollama not available. Skipping real integration demo.")
        print("Install Ollama from https://ollama.ai to try this demo.")
        return

    print("Testing with real Ollama model...")
    print()

    # Ask a question that often triggers evasive responses
    result = llm.chat(
        "Why does my Python function return None instead of the expected value?",
        show_health=True
    )

    if result['health']['status'] != 'healthy':
        print("\n‚ÑπÔ∏è  If drift was detected, the next response will include guidance.")
        result2 = llm.chat(
            "Please answer my specific question: why is it returning None?",
            show_health=True
        )


def show_integration_code():
    """Show the minimal code needed to integrate."""
    print("\n" + "="*80)
    print("MINIMAL INTEGRATION CODE")
    print("="*80)

    code = """
# Minimal integration (10 lines of code)

from conversation_health_tracker import ConversationHealthTracker
import ollama

tracker = ConversationHealthTracker()

def chat(user_msg, system_prompt=""):
    # Get LLM response
    response = ollama.chat(
        model='llama3.2',
        messages=[
            {'role': 'system', 'content': system_prompt},
            {'role': 'user', 'content': user_msg}
        ]
    )
    assistant_msg = response['message']['content']

    # Check health
    tracker.add_turn('user', user_msg)
    health = tracker.add_turn('assistant', assistant_msg)

    # Get guidance if needed
    if health.get('alerts'):
        print("[ALERT]", health['status'])
        return assistant_msg, tracker.generate_coaching_summary()

    return assistant_msg, None

# Use it
response, guidance = chat("Why did X fail?")
if guidance:
    # Next turn includes guidance
    response, _ = chat("Same question", system_prompt=guidance)
"""

    print(code)
    print("\nThat's it! 10 lines to add conversation health monitoring to any local LLM.")


if __name__ == "__main__":
    print("="*80)
    print("LOCAL LLM + CONVERSATION HEALTH TRACKER")
    print("="*80)
    print()
    print("This demo shows conversation health tracking with local LLMs.")
    print("Both the LLM (Ollama) and tracker run 100% locally, zero cloud calls.")
    print()

    # Run demos
    demo_evasion_pattern()
    demo_healthy_conversation()
    demo_with_real_ollama()
    show_integration_code()

    print("\n" + "="*80)
    print("DEMO COMPLETE")
    print("="*80)
    print("\nKey Takeaways:")
    print("  1. Tracker runs alongside your local LLM (Ollama, LMStudio, etc.)")
    print("  2. Adds ~50ms overhead (negligible compared to LLM generation)")
    print("  3. Detects evasion, fragmentation, and other problematic patterns")
    print("  4. Injects guidance into system prompt when needed")
    print("  5. 100% local, zero cloud dependencies, zero ongoing costs")
    print("\nNext steps:")
    print("  ‚Ä¢ Install Ollama: https://ollama.ai")
    print("  ‚Ä¢ Run: python ollama_integration_example.py")
    print("  ‚Ä¢ Or integrate into your existing local LLM setup")
